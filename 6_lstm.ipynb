{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string((train_batches.next())))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits = logits, labels = tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [],
   "source": [
    "def train_lstm(num_steps = 7001, summary_frequency = 100):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print('Initialized')\n",
    "      mean_loss = 0\n",
    "      for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "          feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "          if step > 0:\n",
    "            mean_loss = mean_loss / summary_frequency\n",
    "          # The mean loss is an estimate of the loss over the last few batches.\n",
    "          print(\n",
    "            'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "          mean_loss = 0\n",
    "          labels = np.concatenate(list(batches)[1:])\n",
    "          print('Minibatch perplexity: %.2f' % float(\n",
    "            np.exp(logprob(predictions, labels))))\n",
    "          if step % (summary_frequency * 10) == 0:\n",
    "            # Generate some samples.\n",
    "            print('=' * 80)\n",
    "            for _ in range(5):\n",
    "              feed = sample(random_distribution())\n",
    "              sentence = characters(feed)[0]\n",
    "              reset_sample_state.run()\n",
    "              for _ in range(79):\n",
    "                prediction = sample_prediction.eval({sample_input: feed})\n",
    "                feed = sample(prediction)\n",
    "                sentence += characters(feed)[0]\n",
    "              print(sentence)\n",
    "            print('=' * 80)\n",
    "          # Measure validation set perplexity.\n",
    "          reset_sample_state.run()\n",
    "          valid_logprob = 0\n",
    "          for _ in range(valid_size):\n",
    "            b = valid_batches.next()\n",
    "            predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "            valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "          print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "            valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293301 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.93\n",
      "================================================================================\n",
      "len pboendxeeodne nowspyepjio   rereevre gs p rsvnhqbneoaomf oqgdhm el ot  qce z\n",
      "lnphopl rmdnysgwemfzup  nnflexgcszctxeorx vnu mcfotegbs bwksnubhdp fozsclxlmkse \n",
      "lrek fexsl snrvrdoodmelxouv tgnc bt vxeaj o mwn rac kito ve  xzzsdaeks hlanp sek\n",
      " qwyoosm aamdfnn opjltew eonsbtxkrwyaeiz arlewgsfpratiy vivrhtwzelrarw  onozhxcm\n",
      "t spc bvs cfbcm jopfjm  p rsaddrbmlizolhpemp y md nqy  sgzktflymklcuzr   za crok\n",
      "================================================================================\n",
      "Validation set perplexity: 20.29\n",
      "Average loss at step 100: 2.587937 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.93\n",
      "Validation set perplexity: 11.34\n",
      "Average loss at step 200: 2.264124 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.40\n",
      "Validation set perplexity: 10.04\n",
      "Average loss at step 300: 2.105296 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 8.23\n",
      "Average loss at step 400: 2.029294 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 500: 1.972610 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.75\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 600: 1.937096 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 700: 1.872501 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 800: 1.843372 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 900: 1.848964 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 1000: 1.815581 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "================================================================================\n",
      "baten inlestinghage and to at reseks dee regalig his one nine nine five zero zer\n",
      "nom of five a prosiencation of the race astemers forred a repering and bycare of\n",
      "jernel apous in alangiced tho a anowster engerent sarbacrice over not compuented\n",
      " pany thel por maver menerists in one zero eight linoy oromistion m isqud subhan\n",
      "logince state jown fron imoothand sure at the comliver in in and two tent the in\n",
      "================================================================================\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1100: 1.806313 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 1200: 1.768001 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1300: 1.757926 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 1400: 1.738239 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1500: 1.747124 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1600: 1.729889 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1700: 1.712265 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1800: 1.703217 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1900: 1.698176 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 2000: 1.692782 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "x it the resences of chasius beward a zeur out in nine eur one nine zero eut ele\n",
      "becting nater aften interpice that enfh defithc harain confioned them germalse o\n",
      "joning the marriee to ele to themelet the prodeciateing wester event p of deatts\n",
      "g hostembio voyorh mochines one is care red defer peayly catembly lind s to oper\n",
      "der see in open gree staishign earf the  ledement theli if sict the edotited add\n",
      "================================================================================\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 2100: 1.681661 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 2200: 1.658733 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2300: 1.683730 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2400: 1.689724 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2500: 1.687329 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2600: 1.657999 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2700: 1.637738 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2800: 1.654490 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 2900: 1.673366 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 3000: 1.687264 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "================================================================================\n",
      "notked by an the isal truids glrows to bewnilett octill achnas flin ar niver has\n",
      "violo the offer mintacins its the the euthoure offerring jay be kanned the latte\n",
      "n stite are fingly not system b obgochi hemoceloner po connect groum of the its \n",
      "a colycrist by cardity octarity by nowe colonce othelcer also stan partia ng loc\n",
      "genetholinanteig of the choniaking house volo alsar no mellow toro spay that act\n",
      "================================================================================\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3100: 1.675028 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3200: 1.649870 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3300: 1.643934 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3400: 1.634094 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3500: 1.654365 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3600: 1.607370 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3700: 1.618977 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3800: 1.625640 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3900: 1.601240 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 4000: 1.617953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "================================================================================\n",
      "winelso of itay bum of withis the made bove and ko of canohulse a discomentbe af\n",
      "oug the one zero found interneld million on new imposed every armokic skave band\n",
      "ing husct con that tackenus villoton dither shorttisprant hemong tage recointen \n",
      "m sun hom two k must to neaga sath micismmbrai fightsimia cafitted which two ong\n",
      "mists oxtested measion pall fa the pedions and lirk mochaddacing of a holeug lon\n",
      "================================================================================\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4100: 1.629892 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 4200: 1.647215 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4300: 1.665540 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4400: 1.671855 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4500: 1.636442 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4600: 1.624975 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4700: 1.605215 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4800: 1.593468 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4900: 1.599037 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5000: 1.589618 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "prayer common commet for hadvess noccio for how presedure a gombations work alon\n",
      "manyles quateck consciousness god the nezan this germancinail maronister becauso\n",
      "fice in backs of was the mokel dascress been as modern of a the chuding the prov\n",
      "ys the living ors of seduals scotland this highing in cannorsan the party and al\n",
      "que eight seven three one zero zero one nine five nita botzshur while they expec\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5100: 1.607164 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5200: 1.606984 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5300: 1.595147 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5400: 1.579717 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5500: 1.596544 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5600: 1.586921 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5700: 1.588091 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5800: 1.594416 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5900: 1.565938 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6000: 1.582584 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "able pubulativil flaineover shilling to heelet a stares to there is of acctway t\n",
      "oundaceine the sulder in an of palpans of m from over systerentually cchanpord s\n",
      "hampting systems of s which shone stittonad and practerad is diefiton emerent an\n",
      "blislam although puble lated producted cat of the be schemated estemse s brow co\n",
      "wards that this to beble top as usit a in one of parties the schootional foreing\n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6100: 1.589043 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6200: 1.592431 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 6300: 1.575268 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6400: 1.574199 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6500: 1.557735 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6600: 1.576172 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6700: 1.599288 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6800: 1.588664 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6900: 1.592110 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.01\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 7000: 1.598581 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "================================================================================\n",
      "y distoris and electery casula ng theres i scious momes langy the the still thin\n",
      "lo been s spesial by begus for do web band in a semorary type connettisos lorm d\n",
      "que daming bience discopbes on a ro ane ngver were the first assays following ro\n",
      "letain mocression of though canave that assumblys many agaissons preber s is jon\n",
      "hens there prichion used leadous the commage who was of britaking is programent \n",
      "================================================================================\n",
      "Validation set perplexity: 4.29\n",
      "CPU times: user 3min 24s, sys: 1min 26s, total: 4min 50s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%time train_lstm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # 将i, f, c, o几个矩阵拼成一个\n",
    "  all_x = tf.concat([ix, fx, cx, ox], 1)\n",
    "  all_m = tf.concat([im, fm, cm, om], 1)\n",
    "  all_b = tf.concat([ib, fb, cb, ob], 1)\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    all_mul = tf.matmul(i, all_x) + tf.matmul(o, all_m) + all_b\n",
    "    all_input, all_forget, update, all_output = tf.split(all_mul, 4, 1)\n",
    "    input_gate = tf.sigmoid(all_input)\n",
    "    forget_gate = tf.sigmoid(all_forget)\n",
    "    output_gate = tf.sigmoid(all_output)\n",
    "#     input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#     forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "#     output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits = logits, labels = tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292373 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.91\n",
      "================================================================================\n",
      "fibyt zioq a wqw nb ytcnnjenid oiue  ecnf jxedh dmiexohqeayjazptsbqxtcydukyzymj \n",
      " bmijyd   teotejmetseidy spnpkdvj e dfayhlur p qqovuv u ll nitkiuzed dzb n drrnp\n",
      "etrvuesfqztlcc tehjnoeth  bjslbp   s oxgnxckzub rya  s cmixt  vdilv jnfshmtsgbkd\n",
      "wxoyg ym jmpp msyauisfzwuqtnsii yxzk pojbae c afsi tmg ey hcarmoeyaunt on yvh yt\n",
      "eameojv nifac xgn waaekarwlr jlhjrdquikerfwibtnktojsfzwonte  i osqwjianfw l ocka\n",
      "================================================================================\n",
      "Validation set perplexity: 20.49\n",
      "Average loss at step 100: 2.611551 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.70\n",
      "Validation set perplexity: 10.81\n",
      "Average loss at step 200: 2.265306 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.25\n",
      "Validation set perplexity: 9.16\n",
      "Average loss at step 300: 2.109133 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.40\n",
      "Validation set perplexity: 8.24\n",
      "Average loss at step 400: 2.048444 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.47\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 500: 1.977303 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.92\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 600: 1.919856 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 700: 1.874582 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 800: 1.852024 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 900: 1.867377 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1000: 1.817525 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "================================================================================\n",
      "orne to a c refaid sthird cospect is state of austuc soual the partite to to agp\n",
      "sisty agrainmt litenus to explicatie can reak geor sig has chi zimun detisus nri\n",
      "ymented clust lame this swone place spinstla form tepkerst speck advite cha ligh\n",
      "ise imentherist prixt miviots one predqure oring and the firtilled grepissites w\n",
      "w test scictes conpicatife this as of prudition of ialle stume of x s zalty adsi\n",
      "================================================================================\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1100: 1.806465 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1200: 1.786710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1300: 1.773401 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1400: 1.742047 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1500: 1.748171 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1600: 1.720395 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1700: 1.704279 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 1800: 1.693677 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1900: 1.708718 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2000: 1.715051 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "================================================================================\n",
      "gy b vajer bach aftuliture cared the probatulity mate complated the ppengars by \n",
      "y toliem that car to rain anthip leagle the dues commont that the geth whorth in\n",
      "zaris leducation influsculed arainty procsip prissicio xaid are inventues the th\n",
      "jeldan hage brittrout flagy and which with a contramernam capfing facized which \n",
      "wevem one his a flistionals things the cladmands stembles part yarvelly sorth br\n",
      "================================================================================\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2100: 1.688585 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2200: 1.653970 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2300: 1.675566 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2400: 1.652556 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2500: 1.663251 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2600: 1.667486 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2700: 1.656380 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2800: 1.676101 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2900: 1.675264 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3000: 1.653840 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "================================================================================\n",
      "ly in upp swartony even docendiage it to natoobos witho the defones ratts rckes \n",
      "zerackogy invondance state as kwich or islonua games that place water the s also\n",
      "ons charicis caulation ponnal was six the that thirdg to the which wrik of co in\n",
      "waility of tames car itmountain larsian in to apportber expents the northejs jun\n",
      "quition dispible mmy guisty aborthoust momourating in that the vale mambical to \n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3100: 1.650750 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3200: 1.637318 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3300: 1.624848 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3400: 1.618949 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3500: 1.609489 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 3600: 1.634092 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3700: 1.623873 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 3800: 1.620467 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3900: 1.582606 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4000: 1.639305 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "der requiress in the earl a othering telde bbbion this a new matt that love is d\n",
      "y for martime in the giftherich coor the well one nine seven seven zero zero zer\n",
      "ges memodnal worse airresm drencages hoed aroue dare have alue burse jere genela\n",
      "fice the lodo seven somece desengraful specks kany pasay becoge two feduced nist\n",
      "ul ceble one eight yelece with actid firm  but bare rigients when to spe the gru\n",
      "================================================================================\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4100: 1.634554 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4200: 1.618299 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4300: 1.637412 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4400: 1.629647 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4500: 1.616190 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4600: 1.605992 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4700: 1.623109 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4800: 1.645997 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4900: 1.620320 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5000: 1.611639 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "ward disike fairstic butternations b l on five one zero zero s d  indorial linis\n",
      "ne coldengated to appincing companist rucessalandrain fill ignine from party and\n",
      "like spsword a long up out the out and was buarable affeed to the estand his but\n",
      "der at one nine s alvay those nine stige usent t pirting the rund of scien the b\n",
      "on be cousegn mosonuality to budaday core otyer in ma nobs eight in the wilon sp\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5100: 1.589629 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5200: 1.584723 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5300: 1.601135 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5400: 1.599117 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5500: 1.578381 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5600: 1.589023 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5700: 1.567375 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5800: 1.586824 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5900: 1.578095 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6000: 1.571805 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "================================================================================\n",
      "guarbarg in or cali and respiblis are autyp arganamonical port ne from a musters\n",
      "x the in systemer he holocial barab bromily of uniffed is a sociating data playe\n",
      "zer added by a propstest gro two to hi in one men the fronclale between we be co\n",
      "derf the two in four this ordine which inte noonce night mad allianal yeat dout \n",
      "manguar wen and ppops of a kingal polising the lige in breatter ccussolus payos \n",
      "================================================================================\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6100: 1.581615 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6200: 1.566678 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6300: 1.580338 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6400: 1.598347 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6500: 1.595399 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6600: 1.573699 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6700: 1.617112 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6800: 1.617241 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6900: 1.579695 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 7000: 1.577760 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "jeching shon hem three marilley on parrystans is one nine three three zero secte\n",
      "twhere referents eight nine dman caric at strock had its eneff ne ow voll ky nat\n",
      "warny their rudian pokerate advatomilar to in amerocor teleces fistar cl manspns\n",
      "feronic the brign one years phamoon is war salnor american one four two excherli\n",
      "zust electrostchew support matule utarritives used in the teak fewffien comedy i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.44\n",
      "CPU times: user 4min 1s, sys: 1min 5s, total: 5min 6s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%time train_lstm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还不是很明白tf里面embedding的使用， to be continued。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be continued。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
