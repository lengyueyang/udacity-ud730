{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 画图用\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对2_fullyconnected里面用过的logistic和神经网络， 用L2泛化，可以提高test精确度。我们先从logistic开始。由于L2泛化会引入一个新的参数lambda，我们引入一个新的tf变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  reg_lambda = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels)) + reg_lambda * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_with_l2_loss(reg_lambda_value, processing = False):\n",
    "  num_steps = 3001\n",
    "  \n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized. rain on lambda parameter = %.5lf\" % reg)\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, reg_lambda: reg_lambda_value}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      if (processing and step % 500 == 0):\n",
    "        print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "        print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "          valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "    \n",
    "    return accuracy(valid_prediction.eval(), valid_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized. rain on lambda parameter = 0.07278\n",
      "Minibatch loss at step 0: 314.507751\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 12.2%\n",
      "Minibatch loss at step 500: 1.160237\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1000: 1.165943\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1500: 1.058612\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 2000: 0.957208\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2500: 1.068618\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 3000: 0.986193\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 77.1%\n",
      "Test accuracy: 81.8%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77.099999999999994"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_l2_loss(0.1, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "又引入了一个超参数， 不知道该设置多少，随便设置一个值0.1， 发现效果并没有提升， 反而下降了。我们搜索一下， 看哪个参数比较好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized. rain on lambda parameter = 0.00010\n",
      "Test accuracy: 90.1%\n",
      "Initialized. rain on lambda parameter = 0.00030\n",
      "Test accuracy: 89.6%\n",
      "Initialized. rain on lambda parameter = 0.00090\n",
      "Test accuracy: 90.8%\n",
      "Initialized. rain on lambda parameter = 0.00270\n",
      "Test accuracy: 91.3%\n",
      "Initialized. rain on lambda parameter = 0.00809\n",
      "Test accuracy: 89.1%\n",
      "Initialized. rain on lambda parameter = 0.02427\n",
      "Test accuracy: 86.7%\n",
      "Initialized. rain on lambda parameter = 0.07278\n",
      "Test accuracy: 83.8%\n",
      "tune hyper paramer reg_lambda over.\n"
     ]
    }
   ],
   "source": [
    "reg_val = [pow(10, i) for i in np.arange(-4, -1, 0.477)] # 按Andrew Ng大神的经验， 调参数时候3倍3倍的增长， 大概就是0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1...这样调\n",
    "accuracy_val = []\n",
    "\n",
    "for reg in reg_val:\n",
    "  valid_accuracy = train_with_l2_loss(reg)\n",
    "  accuracy_val.append(valid_accuracy) # 这里要用valid_dataset不能用test_dataset， 否则可能造成overfitting。valid_dataset的作用就是拿来调超参数的\n",
    "        \n",
    "print('tune hyper paramer reg_lambda over.')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAF4CAYAAAA1w9ECAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZpbqH3P1Rkyo4FuUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1163fd7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(reg_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Validation accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现reg_lambda在0.001左右的时候最大，test accuracy在87%左右，确实有所提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized. rain on lambda parameter = 0.07278\n",
      "Minibatch loss at step 0: 23.721201\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 15.6%\n",
      "Minibatch loss at step 500: 2.869463\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 1000: 1.683140\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 1500: 0.923806\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2000: 0.764704\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2500: 0.827983\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 3000: 0.622134\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.3%\n",
      "Test accuracy: 86.5%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82.299999999999997"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_l2_loss(0.001, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们看看对于有一层隐藏层的神经网络，时候有效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_of_hidden_layer = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  reg_lambda = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, num_of_hidden_layer]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_of_hidden_layer]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_of_hidden_layer, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  train_hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(train_hidden_layer, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels)) + \\\n",
    "        reg_lambda * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_hidden_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(valid_hidden_layer, weights2) + biases2)\n",
    "  test_hidden_layer = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_hidden_layer, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized. rain on lambda parameter = 0.07278\n",
      "Minibatch loss at step 0: 31864.312500\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 37.0%\n",
      "Minibatch loss at step 500: 1.473051\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 1000: 1.453134\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 1500: 1.370838\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2000: 1.279009\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 2500: 1.363197\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 3000: 1.339753\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 77.3%\n",
      "Test accuracy: 81.6%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77.299999999999997"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_l2_loss(0.1, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也是将超参数reg_lambda随便设置一个值0.1， 可以看到，跟之前相比(90%)，test accuracy也是降低了。同样， 我们还是来搜索一下reg_lambda看看那个能让valid_accuracy最高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized. rain on lambda parameter = 0.00010\n",
      "Test accuracy: 88.9%\n",
      "Initialized. rain on lambda parameter = 0.00030\n",
      "Test accuracy: 88.6%\n",
      "Initialized. rain on lambda parameter = 0.00090\n",
      "Test accuracy: 91.4%\n",
      "Initialized. rain on lambda parameter = 0.00270\n",
      "Test accuracy: 91.0%\n",
      "Initialized. rain on lambda parameter = 0.00809\n",
      "Test accuracy: 89.4%\n",
      "Initialized. rain on lambda parameter = 0.02427\n",
      "Test accuracy: 86.5%\n",
      "Initialized. rain on lambda parameter = 0.07278\n",
      "Test accuracy: 83.9%\n",
      "tune hyper paramer reg_lambda over.\n"
     ]
    }
   ],
   "source": [
    "reg_val = [pow(10, i) for i in np.arange(-4, -1, 0.477)] # 按Andrew Ng大神的经验， 调参数时候3倍3倍的增长， 大概就是0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1...这样调\n",
    "accuracy_val = []\n",
    "\n",
    "for reg in reg_val:\n",
    "  valid_accuracy = train_with_l2_loss(reg)\n",
    "  accuracy_val.append(valid_accuracy) # 这里要用valid_dataset不能用test_dataset， 否则可能造成overfitting。valid_dataset的作用就是拿来调超参数的\n",
    "        \n",
    "print('tune hyper paramer reg_lambda over.')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAF4CAYAAAA1w9ECAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZpbqH3P1Rkyo4FuUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116430470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(reg_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Validation accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟logistic时候几乎一样， 也是在0.0015多一点的地方valid_accuracy最高， 对应的test_accuracy为92.0%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized. rain on lambda parameter = 0.07278\n",
      "Test accuracy: 92.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "86.5"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_l2_loss(0.0015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看看如果没有regularization，而训练数据又很少的话，会怎样的overfitting！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_of_hidden_layer = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, num_of_hidden_layer]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_of_hidden_layer]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_of_hidden_layer, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  train_hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(train_hidden_layer, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))# + 0.31 * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_hidden_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(valid_hidden_layer, weights2) + biases2)\n",
    "  test_hidden_layer = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_hidden_layer, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 336.788269\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 40.9%\n",
      "Minibatch loss at step 2: 989.343384\n",
      "Minibatch accuracy: 45.3%\n",
      "Validation accuracy: 32.6%\n",
      "Minibatch loss at step 4: 472.988098\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 56.5%\n",
      "Minibatch loss at step 6: 7.091978\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 8: 0.996944\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 10: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 12: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 14: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 16: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 18: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 67.5%\n",
      "Test accuracy: 71.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "#     offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = 10 # 这里写为固定值，说明batch_data每次都是取的同一批， 总共只有batch_size个数据在训练， 所以很容易overfitting\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于有太多的参数而只有少量数据，又没有regularization，我们可以看到train_accuracy达到100%了，valid_accuracy则维持在67%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_of_hidden_layer = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, num_of_hidden_layer]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_of_hidden_layer]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_of_hidden_layer, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  train_hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop_out = tf.nn.dropout(train_hidden_layer, 0.5)\n",
    "  logits = tf.matmul(drop_out, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_hidden_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(valid_hidden_layer, weights2) + biases2)\n",
    "  test_hidden_layer = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_hidden_layer, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 536.540710\n",
      "Minibatch accuracy: 14.8%\n",
      "Validation accuracy: 38.3%\n",
      "Minibatch loss at step 2: 1040.059326\n",
      "Minibatch accuracy: 35.2%\n",
      "Validation accuracy: 50.4%\n",
      "Minibatch loss at step 4: 380.764893\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 61.8%\n",
      "Minibatch loss at step 6: 34.103321\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 68.6%\n",
      "Minibatch loss at step 8: 0.506745\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.4%\n",
      "Minibatch loss at step 10: 3.786833\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 72.9%\n",
      "Minibatch loss at step 12: 0.329448\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 14: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.7%\n",
      "Minibatch loss at step 16: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 18: 1.011385\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 72.7%\n",
      "Test accuracy: 75.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "#     offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = 10 # 这里写为固定值，说明batch_data每次都是取的同一批， 总共只有batch_size个数据在训练， 所以很容易overfitting\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到有了dropout后，train_accuracy很难达到100%了。并且valid_accuracy和test_accuracy也提高了5%左右。 大家可以试试不同的dropout参数会有什么结果，另外大家有兴趣的也可以测试一下L2 regularization的效果怎样。\n",
    "\n",
    "据说dropout是state-of-the-art的regularization技术， 如果dropout都没用， 估计是你的神经网络太复杂而数据太少了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_of_hidden_layer1 = 1024\n",
    "num_of_hidden_layer2 = 100\n",
    "reg_lambda = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  \n",
    "  # Variables.\n",
    "  # 参数初始化相当重要！这里如果不这样设置stddev的话几乎训练不动， 简单解释一下理由：\n",
    "  # 如果只是简单的标准正太分布，大量的小参数累加起来会的到一个绝对值很大的数， 而logistic(x) = 1 / (1 + exp(-x))\n",
    "  # 在x绝对值很大的时候要么等于1（x为很大的正数，如10）要么等于0（x为很小的负数，比如-10），在那些点导数几乎为0\n",
    "  # 所以很难训练。 而这样设置参数的stddev之后，大量参数累加起来之后大概是符合标准正太分布的，所以logistic(x)在这些点比较容易训练。\n",
    "  weights1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, num_of_hidden_layer1],\n",
    "      stddev = np.sqrt(2.0 / (image_size * image_size))))\n",
    "  biases1 = tf.Variable(tf.zeros([num_of_hidden_layer1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_of_hidden_layer1, num_of_hidden_layer2], \n",
    "                       stddev = np.sqrt(2.0 / num_of_hidden_layer1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_of_hidden_layer2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_of_hidden_layer2, num_labels],\n",
    "                       stddev = np.sqrt(2.0 / num_of_hidden_layer2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  \n",
    "  # Training computation.\n",
    "  train_hidden_layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  train_hidden_layer2 = tf.nn.relu(tf.matmul(train_hidden_layer1, weights2) + biases2)\n",
    "#   drop_out = tf.nn.dropout(train_hidden_layer, 0.5)\n",
    "#   logits = tf.matmul(drop_out, weights2) + biases2\n",
    "  logits = tf.matmul(train_hidden_layer2, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels)) + \\\n",
    "        reg_lambda * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  decay_step = 1000\n",
    "  decay_rate = 0.65\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, decay_step, decay_rate, staircase = True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_hidden_layer1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_hidden_layer2 = tf.nn.relu(tf.matmul(valid_hidden_layer1, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden_layer2, weights3) + biases3)\n",
    "  test_hidden_layer1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_hidden_layer2 = tf.nn.relu(tf.matmul(test_hidden_layer1, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_hidden_layer2, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.237680\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 41.9%\n",
      "Minibatch loss at step 500: 0.955061\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 1000: 0.742591\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 1500: 0.507514\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 2000: 0.392957\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2500: 0.387412\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 3000: 0.306398\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 3500: 0.304873\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 4000: 0.284234\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 4500: 0.261848\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 5000: 0.260522\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 5500: 0.269288\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 6000: 0.239695\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 6500: 0.242926\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 7000: 0.231165\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 7500: 0.226236\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 8000: 0.235907\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 8500: 0.219120\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 9000: 0.259469\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 87.2%\n",
      "Test accuracy: 93.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来试试用dropout呢。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_of_hidden_layer1 = 1024\n",
    "num_of_hidden_layer2 = 256\n",
    "num_of_hidden_layer3 = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  batch = tf.Variable(0)\n",
    "  \n",
    "  # Variables.\n",
    "  # 参数初始化相当重要！这里如果不这样设置stddev的话几乎训练不动， 简单解释一下理由：\n",
    "  # 如果只是简单的标准正太分布，大量的小参数累加起来会的到一个绝对值很大的数， 而logistic(x) = 1 / (1 + exp(-x))\n",
    "  # 在x绝对值很大的时候要么等于1（x为很大的正数，如10）要么等于0（x为很小的负数，比如-10），在那些点导数几乎为0\n",
    "  # 所以很难训练。 而这样设置参数的stddev之后，大量参数累加起来之后大概是符合标准正太分布的，所以logistic(x)在这些点比较容易训练。\n",
    "  weights1 = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, num_of_hidden_layer1],\n",
    "      stddev = np.sqrt(2.0 / (image_size * image_size))))\n",
    "  biases1 = tf.Variable(tf.zeros([num_of_hidden_layer1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_of_hidden_layer1, num_of_hidden_layer2], \n",
    "                       stddev = np.sqrt(2.0 / num_of_hidden_layer1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_of_hidden_layer2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_of_hidden_layer2, num_of_hidden_layer3], \n",
    "                       stddev = np.sqrt(2.0 / num_of_hidden_layer2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_of_hidden_layer3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_of_hidden_layer3, num_labels],\n",
    "                       stddev = np.sqrt(2.0 / num_of_hidden_layer3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  \n",
    "  # Training computation.\n",
    "  train_hidden_layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  dropout1 = tf.nn.dropout(train_hidden_layer1, 0.5)\n",
    "  train_hidden_layer2 = tf.nn.relu(tf.matmul(dropout1, weights2) + biases2)\n",
    "  dropout2 = tf.nn.dropout(train_hidden_layer2, 0.5)\n",
    "  train_hidden_layer3 = tf.nn.relu(tf.matmul(dropout2, weights3) + biases3)\n",
    "  dropout3 = tf.nn.dropout(train_hidden_layer3, 0.5)\n",
    "  logits = tf.matmul(dropout3, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay\n",
    "  decay_step = 4000\n",
    "  decay_rate = 0.65\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, decay_step, decay_rate, staircase = True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_hidden_layer1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_hidden_layer2 = tf.nn.relu(tf.matmul(valid_hidden_layer1, weights2) + biases2)\n",
    "  valid_hidden_layer3 = tf.nn.relu(tf.matmul(valid_hidden_layer2, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden_layer3, weights4) + biases4)\n",
    "  test_hidden_layer1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_hidden_layer2 = tf.nn.relu(tf.matmul(test_hidden_layer1, weights2) + biases2)\n",
    "  test_hidden_layer3 = tf.nn.relu(tf.matmul(test_hidden_layer2, weights3) + biases3)    \n",
    "  test_prediction = tf.nn.softmax(tf.matmul(test_hidden_layer3, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.661277\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 23.3%\n",
      "Minibatch loss at step 500: 0.693966\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 1000: 0.480082\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 1500: 0.566766\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 2000: 0.367176\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 2500: 0.519465\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 3000: 0.648941\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 3500: 0.350502\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 4000: 0.463804\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 4500: 0.375013\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 5000: 0.498090\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 5500: 0.343684\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 6000: 0.474748\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 6500: 0.405520\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 7000: 0.366010\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 7500: 0.288743\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 8000: 0.551435\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 8500: 0.460671\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 9000: 0.272105\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 9500: 0.348398\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 10000: 0.444644\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 10500: 0.438574\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 11000: 0.467367\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 11500: 0.724781\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 12000: 0.503140\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 12500: 0.383238\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 13000: 0.414903\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 13500: 0.162809\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 14000: 0.385775\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 14500: 0.225305\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 15000: 0.277148\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 15500: 0.307475\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 16000: 0.421268\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 16500: 0.353473\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 17000: 0.359882\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 17500: 0.338621\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 18000: 0.376045\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 18500: 0.327614\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 19000: 0.457179\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 19500: 0.318453\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 20000: 0.282884\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.8%\n",
      "Test accuracy: 95.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到的数据离97%差了不少，试了各种参数都上不去，只能到93%。 仔细对比我只选择了20000个train_dataset并且没有清洗数据数据，可见数据的重要性！正如那句话说的：\n",
    "```\n",
    "It's not who has the best algorithm that wins. It's who has the most data.\n",
    "```\n",
    "\n",
    "我把train_dataset放成20w条，得到的结果是：95.3%。由于清洗数据之后duplicate的只有1000多条，占整个20w条的比例很小，所以我估计结果误差很小，在自己的mac上跑deep NN，CPU飙到600%+，听着风扇狂转，很是心疼，所以我就放弃了尝试，有兴趣的可以自己试一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
